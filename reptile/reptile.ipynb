{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 253059 into shape (200)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\DataWareHouse\\reptile.ipynb 单元格 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/DataWareHouse/reptile.ipynb#W0sZmlsZQ%3D%3D?line=266'>267</a>\u001b[0m         get_movie_tasks\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_with_httpx(base_url\u001b[39m+\u001b[39mproductId,productId)\u001b[39mfor\u001b[39;00m productId \u001b[39min\u001b[39;00m asins]\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/DataWareHouse/reptile.ipynb#W0sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m         \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mwait(get_movie_tasks)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/DataWareHouse/reptile.ipynb#W0sZmlsZQ%3D%3D?line=271'>272</a>\u001b[0m productIds\u001b[39m=\u001b[39mproductIds\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m200\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/DataWareHouse/reptile.ipynb#W0sZmlsZQ%3D%3D?line=273'>274</a>\u001b[0m loop_times\u001b[39m=\u001b[39mproductIds\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/DataWareHouse/reptile.ipynb#W0sZmlsZQ%3D%3D?line=277'>278</a>\u001b[0m \u001b[39m# for i in range(loop_times):\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/DataWareHouse/reptile.ipynb#W0sZmlsZQ%3D%3D?line=278'>279</a>\u001b[0m \u001b[39m#     r=MyReptile()\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/DataWareHouse/reptile.ipynb#W0sZmlsZQ%3D%3D?line=279'>280</a>\u001b[0m \u001b[39m#     #asyncio.run(r.pipeline(productIds[i]))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/DataWareHouse/reptile.ipynb#W0sZmlsZQ%3D%3D?line=294'>295</a>\u001b[0m \u001b[39m#     print('----------------------')\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/DataWareHouse/reptile.ipynb#W0sZmlsZQ%3D%3D?line=295'>296</a>\u001b[0m \u001b[39m#     break\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 253059 into shape (200)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import httpx\n",
    "from lxml import etree\n",
    "from selenium import webdriver\n",
    "import random\n",
    "from fake_useragent import FakeUserAgent\n",
    "import requests\n",
    "import time\n",
    "columns=[\"ASIN\",\"Release date\",\"Genre\",\"Director\",\"Actors\",\"Producers\",\"Starring\",\"Run time\",\"Language\",\"Version\"]\n",
    "\n",
    "\n",
    "base_url=\"https://www.amazon.com/dp/\"\n",
    "\n",
    "productIds=np.load(\"data/productId.npy\",allow_pickle=True)\n",
    "\n",
    "ua=FakeUserAgent()\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "class MyReptile:\n",
    "    columns=[\"ASIN\",\"Release date\",\"Genre\",\"Director\",\"Actors\",\"Producers\",\"Starring\",\"Run time\",\"Language\",\"Version\"]\n",
    "    MAX_EXCEPTION_TIMES=3\n",
    "    TIME_OUT=5\n",
    "    iii=0\n",
    "\n",
    "    #proxy format is as below\n",
    "    #proxy_list = {'http://': 'http://8.134.138.108:8888',}\n",
    "\n",
    "\n",
    "    def clear_all(self):\n",
    "        self.data.clear()\n",
    "        self.fault_asin.clear()\n",
    "        self.neglect_asin.clear()\n",
    "        self.proxy_list.clear()\n",
    "        self.cookie_pool.clear()\n",
    "        self.parse_fault_asin.clear()\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.data=[]\n",
    "        self.fault_asin=[]\n",
    "        self.neglect_asin=[]\n",
    "        self.parse_fault_asin=[]\n",
    "        self.proxy_list=[]\n",
    "        self.cookie_pool=[]\n",
    "        self.generate_proxy_pool()\n",
    "        self.CONCURRENCY_NUMBER=len(self.proxy_list)\n",
    "        self.SEMAPHORE=asyncio.Semaphore(self.CONCURRENCY_NUMBER*10)\n",
    "\n",
    "    def generate_proxy_pool(self):\n",
    "        self.proxy_list.clear()\n",
    "        response=requests.get(\"http://127.0.0.1:5010/all/\").json()[0:5]\n",
    "        self.proxy_list=[proxy['proxy'] for proxy in response]\n",
    "\n",
    "\n",
    "    def parseWhite(self,response,asin):\n",
    "        tree = etree.HTML(response)\n",
    "        keys=tree.xpath('//*[@id=\"detailBullets_feature_div\"]/ul/li[*]/span/span[1]/text()')\n",
    "        keys=[k.split(\"\\u200f\\n\",1)[0].strip() for k in keys]    \n",
    "        values=tree.xpath('//*[@id=\"detailBullets_feature_div\"]/ul/li[*]/span/span[2]/text()')\n",
    "        #get Genre info\n",
    "        key_list=tree.xpath('//*[@id=\"productOverview_feature_div\"]/div/table/tr[*]/td[1]/span/text()')\n",
    "        value_list=tree.xpath('//*[@id=\"productOverview_feature_div\"]/div/table/tr[*]/td[2]/span/text()')\n",
    "    \n",
    "        key_value_zip=zip(key_list,value_list)\n",
    "        for item in key_value_zip:\n",
    "            if \"Genre\"==item[0] or \"Language\"==item[0]:\n",
    "                keys.append(item[0])\n",
    "                values.append(item[1])\n",
    "                \n",
    "        #get version info\n",
    "        version_list=tree.xpath('//*[@id=\"tmmSwatches\"]/ul/li[*]/span/span/span/a/span[1]/text()')\n",
    "        if len(version_list):\n",
    "            keys.append(\"Version\")\n",
    "            values.append(','.join(version_list))\n",
    "\n",
    "        #add asin info\n",
    "        keys.append(\"ASIN\")\n",
    "        values.append(asin)\n",
    "\n",
    "        keys_values_pair=dict(zip(keys,values))\n",
    "\n",
    "        data_single=[]\n",
    "        for key in self.columns:\n",
    "            if key in keys_values_pair.keys():\n",
    "                data_single.append(keys_values_pair[key])\n",
    "            else:\n",
    "                data_single.append(np.nan)\n",
    "\n",
    "        self.data.append(data_single)\n",
    "\n",
    "    def parseBlack(self,response,asin):\n",
    "        tree = etree.HTML(response)\n",
    "        keys=tree.xpath('//*[@id=\"btf-product-details\"]/div[1]/dl[*]/dt/span/text()') \n",
    "        values=[]\n",
    "        #多值属性：//*[@id=\"btf-product-details\"]/div[1]/dl[x]/dd/a \n",
    "        #单值属性：//*[@id=\"btf-product-details\"]/div[1]/dl[x]/dd\n",
    "        for i,key in enumerate(keys):\n",
    "            if key==\"Directors\":\n",
    "                keys[i]='Director'\n",
    "            elif key=='Audio languages':\n",
    "                keys[i]=\"Language\"\n",
    "            mul_val_pattern=f'//*[@id=\"btf-product-details\"]/div[1]/dl[{i+1}]/dd/a/text()'\n",
    "            single_val_pattern=f'//*[@id=\"btf-product-details\"]/div[1]/dl[{i+1}]/dd/text()'\n",
    "            single_val_get=tree.xpath(single_val_pattern) \n",
    "            if not len(single_val_get):\n",
    "                values.append(','.join(tree.xpath(mul_val_pattern)))\n",
    "            else:\n",
    "                values.append(single_val_get[0])\n",
    "\n",
    "        #add asin info\n",
    "        keys.append(\"ASIN\")\n",
    "        values.append(asin)\n",
    "        #runtime\n",
    "        runtime_val=tree.xpath('//*[@id=\"main\"]/div[1]/div/div/div[2]/div[3]/div/div[2]/div[3]/div/span[@data-automation-id=\"runtime-badge\"]/text()')\n",
    "        if len(runtime_val):\n",
    "            keys.append(\"Run time\")\n",
    "            values.append(','.join(runtime_val))\n",
    "        #release time\n",
    "        release_val=tree.xpath('//*[@id=\"main\"]/div[1]/div/div/div[2]/div[3]/div/div[2]/div[3]/div/span[@data-automation-id=\"release-year-badge\"]/text()')\n",
    "        if len(release_val):\n",
    "            keys.append(\"Release date\")\n",
    "            values.append(','.join(release_val))\n",
    "        #genre\n",
    "        genre_val=tree.xpath('//*[@id=\"main\"]/div[1]/div/div/div[2]/div[3]/div/div[2]/div[4]/div/span[*]/a/text()')\n",
    "        if len(genre_val):\n",
    "            keys.append(\"Genre\")\n",
    "            values.append(','.join(genre_val))\n",
    "\n",
    "        #version/format\n",
    "        version_val=tree.xpath('//*[@id=\"btf-product-details\"]/div[2]/div/div/a/span/strong/text()')\n",
    "        if len(version_val):\n",
    "            keys.append(\"Version\")\n",
    "            values.append(','.join(version_val))\n",
    "\n",
    "        keys_values_pair=dict(zip(keys,values))\n",
    "        data_single=[]\n",
    "        for key in self.columns:\n",
    "            if key in keys_values_pair.keys():\n",
    "                data_single.append(keys_values_pair[key])\n",
    "            else:\n",
    "                data_single.append(np.nan)\n",
    "        self.data.append(data_single)\n",
    "\n",
    "    def whiteOrBlack(self,response)->int:\n",
    "        tree = etree.HTML(response)\n",
    "        if not len(tree.xpath('//*[@id=\"nav-search-label-id\"]/text()')):\n",
    "            return 2       \n",
    "        type=tree.xpath('//*[@id=\"nav-search-label-id\"]/text()')[0]\n",
    "        if type=='Movies & TV':\n",
    "            return 0\n",
    "        elif type==\"Prime Video\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "        \n",
    "    def parse(self,response,asin):\n",
    "        judge=self.whiteOrBlack(response)     \n",
    "        try:  \n",
    "            if judge==0:\n",
    "                self.parseWhite(response,asin)\n",
    "            elif judge==1:\n",
    "                self.parseBlack(response,asin)\n",
    "            else:\n",
    "                self.neglect_asin.append(asin)\n",
    "        except:\n",
    "            self.parse_fault_asin.append(asin)\n",
    "\n",
    "    def get_proxy(self):\n",
    "        proxy=random.choice(self.proxy_list)\n",
    "        if len(self.proxy_list):\n",
    "            return {'http://': \"http://\"+proxy}\n",
    "        else:\n",
    "            return{}\n",
    "\n",
    "    def remove_proxy(self,proxy):\n",
    "        try:\n",
    "            self.proxy_list.remove(proxy)      \n",
    "            print(\"proxy removed\")\n",
    "        except:\n",
    "            print(\"remove proxy error\")\n",
    "        finally:\n",
    "            if len(self.proxy_list)<self.CONCURRENCY_NUMBER:\n",
    "                if len(self.proxy_list)>=10:\n",
    "                    self.CONCURRENCY_NUMBER=len(self.proxy_list)\n",
    "                else:\n",
    "                    self.generate_proxy_pool()\n",
    "            \n",
    "\n",
    "    #pool中cookie数量大于20才可随机选择\n",
    "    def get_cookie(self):\n",
    "        if len(self.cookie_pool)>=20:\n",
    "            return random.choice(self.cookie_pool)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def remove_cookie(self,cookie):\n",
    "        try:\n",
    "            self.cookie_pool.remove(cookie)\n",
    "            print(\"remove cookie\")\n",
    "        except:\n",
    "            print(\"remove cookie error\")\n",
    "        \n",
    "\n",
    "    def add_cookie(self,cookie):\n",
    "        if cookie not in self.cookie_pool:\n",
    "            self.cookie_pool.append(cookie)\n",
    "\n",
    "\n",
    "    async def get_with_httpx(self,url,asin):\n",
    "        headers = {\n",
    "          'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36',\n",
    "          'accept-language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7,zh-TW;q=0.6',\n",
    "          'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "          'accept-encoding': 'gzip, deflate, br',\n",
    "          \"Referer\": \"https://www.amazon.com/\",\n",
    "        }\n",
    "        headers[\"User-Agent\"]=ua.random\n",
    "        exception_times=0\n",
    "        while exception_times<self.MAX_EXCEPTION_TIMES:\n",
    "            try:\n",
    "                async with self.SEMAPHORE:\n",
    "                    proxy=self.get_proxy()\n",
    "                    async with httpx.AsyncClient(proxies=proxy,timeout=self.TIME_OUT) as client: \n",
    "                        rand_cookie=self.get_cookie()\n",
    "                        #只给2次机会，第一次要是不行，第二次直接不使用原来的cookie\n",
    "                        for i in range(2):\n",
    "                            try:\n",
    "                                response=await client.get(url=url,headers=headers,cookies=rand_cookie)\n",
    "                                if i==0 and rand_cookie==None:\n",
    "                                    self.add_cookie(response.cookies)\n",
    "                                break                               \n",
    "                            except:\n",
    "                                if i==1:\n",
    "                                    print(asin+\"  \"+\"cookie 2 times\"+\" wrong\")\n",
    "                                    raise \"\"\n",
    "                                else:\n",
    "                                    self.remove_cookie(rand_cookie)\n",
    "                                    rand_cookie=None\n",
    "\n",
    "                        self.parse(response.text,asin)\n",
    "                        self.iii+=1\n",
    "                        print(self.iii)\n",
    "                        #只要走过parse，所有的错误都是解析异常，和请求没有关系！\n",
    "                        #print(proxy[\"http://\"]+\"  \"+asin+\" ok\")\n",
    "                        return\n",
    "            except:\n",
    "                exception_times+=1\n",
    "                #print(asin+\"exception times\"+str(exception_times))\n",
    "                if exception_times==3:\n",
    "                    #print(asin+\" proxy error: \"+proxy[\"http://\"])\n",
    "                    self.remove_proxy(proxy[\"http://\"])\n",
    "        \n",
    "        self.fault_asin.append(asin)\n",
    "        #print(\"append fault asin:\"+asin)\n",
    "\n",
    "    #just for emergency:by using selenium @warning:very low efficiency\n",
    "    def get_with_selenium(self,url,asin):\n",
    "        browser = webdriver.Edge() # 打开浏览器\n",
    "        browser.minimize_window()  # 最小化窗口\n",
    "        response=browser.get(url)#访问相对应链接\n",
    "           \n",
    "\n",
    "    async def pipeline(self,asins):\n",
    "        get_movie_tasks=[self.get_with_httpx(base_url+productId,productId)for productId in asins]\n",
    "        await asyncio.wait(get_movie_tasks)\n",
    "\n",
    "\n",
    "\n",
    "productIds=productIds.reshape(-1,200)\n",
    "\n",
    "loop_times=productIds.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(loop_times):\n",
    "#     r=MyReptile()\n",
    "#     #asyncio.run(r.pipeline(productIds[i]))\n",
    "#     await r.pipeline(productIds[i])\n",
    "#     df=pd.DataFrame(data=r.data,columns=r.columns)\n",
    "#     df_fault=pd.DataFrame(data=r.fault_asin,columns=['ASIN'])\n",
    "#     df_neglect=pd.DataFrame(data=r.neglect_asin,columns=['ASIN'])\n",
    "#     df_parse_fault=pd.DataFrame(data=r.parse_fault_asin,columns=['ASIN'])\n",
    "#     df.to_csv(f'./ans/{i+3}.csv',index=False)\n",
    "#     df_fault.to_csv(f\"./ans/{i+3}fault.csv\",index=False)\n",
    "#     df_neglect.to_csv(f\"./ans/{i+3}neglect.csv\",index=False) \n",
    "#     df_parse_fault.to_csv(f\"./ans/{i+3}parsefault.csv\",index=False) \n",
    "\n",
    "#     print(\"epoch\"+str(i+3))\n",
    "#     print('----------------------')\n",
    "#     print(len(r.proxy_list))\n",
    "#     print(len(r.cookie_pool))\n",
    "#     print('----------------------')\n",
    "#     break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aspect Ratio', 'Is Discontinued By Manufacturer', 'MPAA rating', 'Package Dimensions', 'Director', 'Media Format', 'Run time', 'Release date', 'Actors', 'Language', 'Studio', 'ASIN', 'Number of discs']\n",
      "['ASIN', 'Release date', 'Genre', 'Director', 'Actors', 'Producers', 'Starring', 'Run time', 'Language', 'Version']\n",
      "['B000BMY2M4', 'December 13, 2005', nan, 'Stephen Caffrey, Reynaldo Villalobos', 'Terence Knox, Stephen Caffrey, Tony Becker, Stan Foster, Ramon Franco', nan, nan, '45 hours and 18 minutes', 'Unqualified', 'DVD']\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import numpy as np\n",
    "columns=[\"ASIN\",\"Release date\",\"Genre\",\"Director\",\"Actors\",\"Producers\",\"Starring\",\"Run time\",\"Language\",\"Version\"]\n",
    "with open('sss.html','r',encoding='utf-8') as f:\n",
    "    tree = etree.HTML(f.read())\n",
    "    keys=tree.xpath('//*[@id=\"detailBullets_feature_div\"]/ul/li[*]/span/span[1]/text()')\n",
    "    values=tree.xpath('//*[@id=\"detailBullets_feature_div\"]/ul/li[*]/span/span[2]/text()')\n",
    "    keys=[k.split(\"\\u200f\\n\",1)[0].strip() for k in keys] \n",
    "    print(keys)\n",
    "#get Genre info\n",
    "    key_list=tree.xpath('//*[@id=\"productOverview_feature_div\"]/div/table/tr[*]/td[1]/span/text()')\n",
    "    value_list=tree.xpath('//*[@id=\"productOverview_feature_div\"]/div/table/tr[*]/td[2]/span/text()')\n",
    "    \n",
    "    key_value_zip=zip(key_list,value_list)\n",
    "    for item in key_value_zip:\n",
    "        if \"Genre\"==item[0] or \"Language\"==item[0]:\n",
    "            keys.append(item[0])\n",
    "            values.append(item[1])\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "#get version info\n",
    "    version_list=tree.xpath('//*[@id=\"tmmSwatches\"]/ul/li[*]/span/span/span/a/span[1]/text()')\n",
    "    if len(version_list):\n",
    "            keys.append(\"Version\")\n",
    "            values.append(','.join(version_list))\n",
    "    keys_values_pair=dict(zip(keys,values))\n",
    "\n",
    "    data_single=[]\n",
    "    for key in columns:\n",
    "        if key in keys_values_pair.keys():\n",
    "            data_single.append(keys_values_pair[key])\n",
    "        else:\n",
    "            data_single.append(np.nan)\n",
    "    print(columns)\n",
    "    print(data_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B00000INB0', 'B000BMY2M4', 'B000I9WVQY', 'B0019CYESM',\n",
       "       'B000A8AXLS'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "productIds=np.load(\"data/productId.npy\",allow_pickle=True)\n",
    "productIds[5000:5005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response=requests.get(\"http://www.zdopen.com/PrivateProxy/GetIP/?api=202310301622236652&akey=57ecaf25ad4e9461&fitter=2&timespan=6&type=3\").json()\n",
    "proxysss=[]\n",
    "for ip_port in response['data']['proxy_list']:\n",
    "    proxysss.append(ip_port['ip']+\":\"+str(ip_port['port']))\n",
    "    \n",
    "\n",
    "\n",
    "proxy_list=[\"http://\"+proxy for proxy in proxysss]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reptile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
